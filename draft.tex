\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[colorlinks]{hyperref}
\usepackage{capt-of}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage{float}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\E}{\mathbb{E}}
\newcommand {\bX}{\mathbf{X}}
\newcommand {\bU}{\mathbf{U}}
\newcommand {\bS}{\mathbf{S}}
\newcommand {\bV}{\mathbf{V}}
\newcommand {\bH}{\mathbf{H}}
\newcommand{\mbf}[1]{{\mathbf{#1}}}
\usepackage{listings}
\usepackage{subcaption}


\title{Thesis Draft}
\author{Genglin Liu }
\date{February 2021}

\begin{document}

\maketitle

\section{Introduction}

\begin{algorithm}[H]
\SetAlgoLined
 Inputs: \\
 $X \leftarrow$ input $n \times n$ image \\
 $s \leftarrow d$-dim structured covariate vector\\
 $s_\ell \leftarrow$ the $\ell^{th}$ element of the $d$-dim structured covariate vector\\
 $x_{i,j} \leftarrow$ $p \times p$ patch from the input image centered at $i,j$ \\
 $\kappa \leftarrow$ $p \times p$ convolutional filter \\ 
 \For{every image patch {x}_{i, j}}{
  $\kappa_\ell \leftarrow w_0 + w s_\ell$ \;
  cross correlation to get $z_{i, j} = \mathbf{1}^T({x}_{i, j} \odot \kappa)\mathbf{1} + \beta$\;
%   \eIf{condition}{
%   instructions1\;
%   instructions2\;
%   }{
%   instructions3\;
%   }
 }
 \caption{Learning hybrid convolutional filters}
\end{algorithm}


Learning the hybrid convolutional kernels \\

Shortfuse was originally designed for time-series and temporal signals. For our task, the time series is replaced by grid-like image data.\\

The hybrid convolution maps an input image $x$ of size n x n to a matrix $z$ of size n x n. Each element $z_{i, j}$ of $z$ is computed from a image patch from x. \\

We have $$z_{i, j} = \mathbf{1}^T(\Bar{x}^{i, j} \odot \kappa)\mathbf{1} + \beta$$

where $\Bar{x}$ is the image patch centered at pixel (i, j) of dimension n' x n', $\kappa$ is the convolutional kernel of the same dimension as the patch, $\odot$ denotes the Hadamard product and $\mathbf{1}$ is a matrix of 1's and $\beta$ is the bias term.  

Essentially every element of $z$ comes from a cross-correlation operation, which is done by taking the sum of an element-wise product between the image patch and the kernel

We first obtain several structured covariates such as age, gender, cognitive test scores, etc. Then we incorporate each of them with one convolutional filter. We want every convolutional filters related to each one of them, and then we may have an additional free filter without fusion that just learns like a normal 2d conv kernel. 

\subsection{Hybrid Convolution}

$$\kappa_l = w_0 + w_i s_l$$ 

input of the procedure: image $s$ and structure covariate vector $s$.
Suppose that the images are scaled to squares of $n$ x $n$ dimensions for convenience and the covariate vector $s$ has a dimension of $d$. In substitute of the time axis in a time series, we will just have the size of the image for our task. 

The weights $w_l$ and the bias term $w_0$ are both learnable parameters, and $s_l$ represents the l-th structured covariate used for the corresponding conv filter.  

so we'll have as many filters as we have structured covariates.
As for now we will discuss under the assumption that we're only getting one structured covariate. As we will see in the next section, we will first work with one structured covariate. 

\section{Hybrid CNN layer on CelebA dataset}

\subsection{CelebA Dataset}

We experiment our hybrid convolutional layers on a toy dataset before we move on to the ADNI data and apply it to the MRI image feature extractors.

[talk a little bit about the CelebA dataset]

The toy problem we are solving in this section is to predict a binary facial attribute in the CelebA dataset using deep learning. Later we will investigate whether the shortfused hybrid convolutional layers improve performance.

\subsection{Model and Experiments}
We first used a VGG-16 model that was pretrained on the ImageNet dataset, with the last linear layer modified for binary classification.

1. First tried a resnet-18 pretrained on ImageNet and froze the layers except for the last one. Had about 63\% validation accuracy. 
2. Then replaced pretrained resnet with a pretrained VGG-16. Trained on one epoch over the training set and the val accuracy was about 73\%. The transfer learning does not perform well for either model. 
3. Tried to train vgg-16 from scratch (without pretraining on ImageNet). High memory use and could not perform this locally. Had to use the cluster to compute but the accuracy was horrible - barely over 50\%
4. We finally took a pre-trained VGG-16 but re-trained it on CelebA - aka no parameter was frozen at the beginning and every layer was retrained. This gave the best performance - validation accuracy about 78-79\% on attrativeness attribute prediction. 


\end{document}