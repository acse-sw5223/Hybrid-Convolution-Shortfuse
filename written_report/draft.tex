\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[colorlinks]{hyperref}
\usepackage{capt-of}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage{float}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\E}{\mathbb{E}}
\newcommand {\bX}{\mathbf{X}}
\newcommand {\bU}{\mathbf{U}}
\newcommand {\bS}{\mathbf{S}}
\newcommand {\bV}{\mathbf{V}}
\newcommand {\bH}{\mathbf{H}}
\newcommand{\mbf}[1]{{\mathbf{#1}}}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{setspace}
\onehalfspacing
\usepackage{geometry}


\title{Customized Hybrid Convolutional Layers in Deep Learning Models}
\author{Genglin Liu, Ina Fiterau }
\date{April 2021}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Facial Attractiveness Prediction}
The problem we're investigating here is to predict a binary facial attribute in the CelebA dataset using deep learning. We explore whether shortfuse could yield in better performance than a regular convolutional neural network. 

\subsection{VGG}
I selected the VGG Net [reference] as the baseline model for my experiments. There were a few other popular options to choose from, such as Residual network (ResNet) [reference] or an older network such as AlexNet [reference], but VGG turned out to be the ideal candidate. The reason is that VGG has a very straightforward architecture compared to ResNet which has more complex skip connections between layers. As we start building a new model, it is more reasonable to start with simpler architectures. On the other hand, AlexNet, along with many other older and simpler architectures, are not powerful enough to reach the level of performance that VGG does on image recognition tasks. 

\subsection{Hybrid CNN layer on CelebA dataset}

\subsubsection{CelebA Dataset}

We experiment our hybrid convolutional layers on a toy dataset before we move on to the ADNI data and apply it to the MRI image feature extractors.

We use the CelebFaces dataset[reference] which contains 202,599 face
images of 10,177 identities (celebrities) collected from
the internet and annotated with 40 facial attributes. The dataset is split into 162,770 instances for training and 19,962 instances for testing.

The toy problem we are solving in this section is to predict a binary facial attribute in the CelebA dataset using a deep convolutional network VGG Net. Later we will investigate whether the shortfused hybrid convolutional layers improve performance on the same classification task.

\subsection{Significance and Relevance to Neuroimaging and Alzheimer's Detection}

Alzheimer’s disease (AD)  is a degenerative condition that causes major changes in the brain, leading to cognitive decline and death in a matter of a few years. There is no known effective cure for the Alzheimer's. Many treatments have been tried, immense effort has been expended into clinical trials and nothing has worked so far. The main reason is that, usually by the time Alzheimer's is diagnosed, 60\% of brain matter is gone and the process is irreversible.

Most of the existing clinical trials fail to help identify a cure to the disease and a lot of pharmaceutical companies have invested and an estimated 150 to 300 experimental Alzheimer's treatments have failed to show clinical benefits. 

As an open challenge in healthcare, diagnosing and forecasting the Alzheimer's Disease have been studied a lot by healthcare professionals and data scientists. Current popular forecasting models tend to use hand-crafted volumetric features from brain magnetic resonance imaging (MRI) as input, which may not be as informative as features extracted directly from the scans themselves. 

We propose a novel forecasting framework which takes patient MRI scans as input and utilizes a deep 3D Convolutional Neural Network (CNN) to extract features from a patient’s brain MRIs over multiple visits. We then fuse these features extracted from the neural network with the cognitive test scores of a patient and demographic data. These features are then fed into a Recurrent Neural Network, which can provide an insight into the patient’s disease progression over time. We show that the inclusion of these customised/patient-specific features increases the F1-score, sensitivity, and specificity of forecasting the disease stage of cognitively normal (CN) and Mild Cognitive Impairment (MCI) patients over a horizon of 2 years. The results of our findings are validated on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset.

To further utilize the abundant information we have from the ADNI and TADPOLE[reference] data sets, we introduce a novel technique called ShortFuse[reference]. In simple terms, ShortFuse is a method to fuse information from structured data (such as tabular data of age, height, gender, mass, etc in biomedical applications) into unstructured data (images or time-series data that tend to need deep learning models to extract features from). We propose that ShortFuse is going to help with our study of Alzheimer's Disease forecasting, because we are able to access a rich set of data ranging from cognitive scores, demographic information to the actual volumetric 3-dimensional MRI brain scan images. We speculate that by incorporating ShortFuse on our CNN image feature extractor, we will be able to achieve higher performance in our final end-to-end trainable forecasting model that consists of a CNN image feature extractor and an RNN-based forecasting pipeline. 


\section{Background}

\subsection{ShortFuse: Fusing Structured Covariates with  Unstructured Information}

Fiterau at el.introduced  a  new  method,  called  ShortFuse,  that  incorporates  structured covariates into time series deep learning models, which was shown to improve performance over current state-of-the-art models on the task of forecasting osteoarthritis progression [6].It is common in biomedical applications that when we are analyzing patient records or medical data,  we are likely to obtain structured information such as age,  gender,  height,weight, etc.  It has become popular nowadays for machine learning researchers and data scientists to apply deep learning methods to extract features from unorganized or unstructured data such as medical images or time-series data such as electroencephalography (EEG) signals.  In addition to the exceptional computational capabilities these deep learning models have and the successes they have already brought to the community, Fiterau at el. propose that  all  information  may  be  found  useful  for  prediction  or  classification  tasks,  and  it  is speculated that if the structured covariates also get incorporated into the machine learning models via certain information-fusion techniques, the performance may be higher.This  method  is  called  ShortFuse.   The  practice  of  information  fusion  is  theoretically7
demonstrated [6], and this technique is versatile.  ShortFuse makes no assumptions regarding the structure, dimensionality, or sampling frequency of the time series.  It can be applied to either RNN or CNN model architectures as demonstrated in the article.  It is also evident through  experimenting  that  ShortFuse  matches  or  improves  on  results  obtained  through feature  engineering  performed  by  domain  experts,  and  achieves  state-of-the-art  accuracy with no manual feature engineering.  [6]Concretely, ShortFuse works on the premise that the earlier we integrate the structured covariates into the deep learning model, the more effective it should be in order to produce better feature construction.  Hence for convolutional neural networks, ShortFuse should be applied predominantly although not exclusively on the initial convolutional layers.  In the article this practice is called Hybrid Convolutions.  The CNN example shown in the paper uses dropout, and then it is applied to the sequences in a time window with the structured covariates (age, gender, height, and mass) as parameters.  The authors presents that there can  be  several  convolutional  filters  in  the  architecture,  the  output  of  which  are  pooled,followed  by  an  additional  layer  of  convolutions  which  can  then  use  the  covariates  again.The paper also discussed ShortFuse working with LSTM and their specific gates in further details.

\section{Methodology}

\subsection{Core Algorithm}
\begin{algorithm}[H]
\SetAlgoLined
 Inputs: \\
 $X \leftarrow$ input $n \times n$ image \\
 $s \leftarrow d$-dim structured covariate vector\\
 $s_\ell \leftarrow$ the $\ell^{th}$ element of the $d$-dim structured covariate vector\\
 $x_{i,j} \leftarrow$ $p \times p$ patch from the input image centered at $i,j$ \\
 $\kappa \leftarrow$ $p \times p$ convolutional filter \\ 
 \For{every image patch {x}_{i, j}}{
  $\kappa_\ell \leftarrow w_0 + w s_\ell$ \;
  2D convolution: $z_{i, j} = \mathbf{1}^T({x}_{i, j} \odot \kappa)\mathbf{1} + \beta$\;
 }
\caption{Learning hybrid convolutional filters}
\end{algorithm}

Learning the hybrid convolutional kernels \\

Shortfuse was originally designed for time-series and temporal signals. For our task, the time series is replaced by grid-like image data.\\

The hybrid convolution maps an input image $x$ of size n x n to a matrix $z$ of size n x n. Each element $z_{i, j}$ of $z$ is computed from a image patch from x. \\

We have $$z_{i, j} = \mathbf{1}^T(\Bar{x}^{i, j} \odot \kappa)\mathbf{1} + \beta$$

where $\Bar{x}$ is the image patch centered at pixel (i, j) of dimension n' x n', $\kappa$ is the convolutional kernel of the same dimension as the patch, $\odot$ denotes the Hadamard product and $\mathbf{1}$ is a matrix of 1's and $\beta$ is the bias term.  

Essentially every element of $z$ comes from a cross-correlation operation, which is done by taking the sum of an element-wise product between the image patch and the kernel

We first obtain several structured covariates such as age, gender, cognitive test scores, etc. Then we incorporate each of them with one convolutional filter. We want every convolutional filters related to each one of them, and then we may have an additional free filter without fusion that just learns like a normal 2d conv kernel. 

And the temporal concept is demonstrated through the movement of the convolutional kernels across the image.

\subsection{Hybrid Convolution}

$$\kappa_l = w_0 + w_1 s_l$$ 

input of the procedure: image $s$ and structure covariate vector $s$.
Suppose that the images are scaled to squares of $n$ x $n$ dimensions for convenience and the covariate vector $s$ has a dimension of $d$. In substitute of the time axis in a time series, we will just have the size of the image for our task. 

The weights $w_1$ and the bias term $w_0$ are both learnable parameters, and $s_l$ represents the l-th structured covariate used for the corresponding conv filter.  

so we'll have as many filters as we have structured covariates.
As for now we will discuss under the assumption that we're only getting two structured covariate. As we will see in the next section, we will first work with two structured covariate. (male and female) 

\subsection{Experimental Designs}


\subsection{Baseline experiments}
First order of business is to get a reasonable performance on facial attractiveness prediction using a regular convolutional neural network. According to the literature I've come across, it is possible to achieve around 83\% accuracy on the attractiveness attribute prediction task [reference!]

The initial idea is to perform simple transfer learning on a popular CNN architecture. Our first attempt was with a residual network. I used a resnet-18 model pretrained on ImageNet and froze the layers except for the last one. This model is a 18-layer deep residual network. I then replaced pretrained resnet with a pretrained VGG-16 model. From the ImageNet benchmark, the 16-layer VGG net does not perform any less well than ResNet, while having a more straightforward architecture. 

After the two initial attempts on transfer learning, my advisor suggested that I should train the network from scratch instead. I had another naive attempt by getting a vanilla VGG network that is not pretrained at all and tried to train from scratch on CelebA. For the first time I realized that training a 16-layer deep network became too much for my local machine due to the high GPU memory usage and could not be performed locally. So most of the experiments from that point on was done on the Gypsum cluster. This attempt ended up being too naive and the accuracy was barely over 50\%.

We concluded that the baseline model needs to have pretrained weights but we'd still train it from scratch on the CelebA images instead of just fine-tuning it. We finally took a pre-trained VGG-16 with Batch Normalization but re-trained it on CelebA - aka no parameter was frozen at the beginning and every layer was retrained. 

\subsection{Hybrid layers}
The next step of this project is to introduce the hybrid convolutional layers. One of the core feature of our project is the implementation of a customized convolutional layer in our deep learning model. Unlike the vanilla convolutional layers, this hybrid convolutional layer takes a structured covariate as parameter, and this covariate will activate the learning of certain weights when its value is non-zero.

Pytorch layers are implemented as classes that extend \textit{nn.Module}, and so we defined two weight matrices and introduce the covariate as a single scalar whose value depends on the sample that goes through. The only other thing we had to do is defining the forward pass. For instance, if the current training image has a label "male", then the covariate parameter of the hybrid layer will be 1 for that sample

This layer will be adapted to 3D tasks (brain MRIs) if needed. For the purpose of our tasks on 2D images of celebrity faces, all the operators such as convolution, batch normalization, and max pooling are done in 2D. 

\subsection{Batch inputs and learning rates}
Once the hybrid layer was build, a few experiments were performed and recorded (see section 4 for details). The preliminary results weren't very satisfactory because the initial implementation of the layer could only take single input and therefore prohibits the use of batch normalization and minibatch gradient descent. Several modifications were made to enable the hybrid network to take batch inputs, and we performed subsequent experiments to test which batch sizes and corresponding learning rates would be best for our classifier. 

\subsection{Early-Fuse vs Later-Fuse}
From the original ShortFuse paper we have seen that it is arguably better to place the hybrid convolutional layers as early as possible, because then the structured information would have a longer influence on the learning. 

However, a counter idea is that at the first convolution, the network is generally still learning very low level features such as edges or blobs, and the structured information may not seem as significant to the learning yet. So one of the other experiments we designed is to move the hybrid layer to the second or third convolution and see if the network will start picking up these higher level features along with the given structured covariates. 

\subsection{More Covariates}
[potentially move this to future work]
In our previous experiments, gender was the only covariate attribute we used to help with attractiveness prediction. The question we then pose is how will another covariate affect the performance. Further, how can we build a hybrid model that is capable of taking an arbitrary number of covariates and flexibly process images along with all of them? This gives us a new direction to extend our model. 

\subsection{Facilities and Hardware}
To accommodate the need for compute resources, most of the computational experiments were done on Gypsum, a multi-GPU compute cluster located at Massachusetts Green High-Performance Computing Cluster (MGHPCC). Specifically, I utilize the cluster by securely connecting to the server from my local computer, then access the remote graphic processing units (GPU) there to perform the experiments. 

In facilitating my own experiments with the 16-layer VGG network and the hybrid version of it, all of the recorded experiments were ran on Nvidia's TitanX-long GPU with 4096MB reserved memory. I ran all of my experiments on a single compute node, i.e. no parallel computing. Most of the experiments involve the network processing the entire training set once and they usually finish between one and two hours. 


\section{Experiments, Implementation Details and Results}

\subsection{Baseline and Preliminary Experiments Without Hybrid Layer}

VGG16 Network pretrained on ImageNet gave the best performance - validation accuracy about 78-79\% on attrativeness attribute prediction. We used mini batches of size 8 for the task.

\subsection{Hybrid Layers with no Batch Inputs}
Before adapting the hybrid convolutional layer to the VGG Net, I first built a simple two-layer hybrid CNN. It took some tricks to make this network work. First I defined different forward passes with different covariate values (1 for male and 0 for female). Then at each iteration only one image can pass through the network so batch size can only be 1. I thought it could not take batch inputs because otherwise we would have multiple images but only one integer for the covariate value for each forward pass. Additionally, I removed all the \textit{nn.Sequential} containers in the implementation because we need to customize forward passes. 

I went on to replace the first convolutional layer in VGG16 with the new hybrid layer that takes as input an extra covariate. Since the naive hybrid layer could not take batch input, I had to remove all the batch normalization layers as well and proceed with the regular VGG. What I did not realize at that point was that batch normalization was essential for this task and it plays a much bigger role in ensuring the stability in the gradient flow than I thought. 

Therefore the first experiment on the modified hybrid VGG didn't perform well at all. The training took more than 3 hours because of the low batch size, and the validation accuracy came out to be 0.52. It was only after several other experiments later I observed that the model encountered gradient problem during training and that during validation phase it was predicting only 0 or 1's, and that's why I kept seeing the 0.52 and 0.48 validation accuracy.

The following experiments were performed to confirm that batch normalization was crucial to this task. \\

\noindent exp1: vgg16\_bn batchsize=16 [val accuracy = 0.783]  \\
exp2: vgg16 batchsize=16 [val accuracy = 0.520] - oscillating training loss \\
exp3: vgg16 bacthsize=1 [val accuracy = 0.520] - unstable training \\
exp4: vgg16\_bn batchsize=1 [val accuracy = 0.480] - gradient vanished \\
exp5: vgg16\_bn batchsize=16 [val accuracy = 0.791] \\

[TODO: make this a table instead]

It can be observed that regardless of batchsize, regular VGG16 network ended up having gradient problem and the training would fail, so did the VGG16-BN network (which applies batch normalization to every convolutional block), but without minibatch inputs. 

\subsection{Hybrid Layers with Batch Inputs}

\subsubsection{Implementation Details of the Hybrid CNN with Batch Normalization}

Since we recognize the importance of providing a detailed explanation of the novel model, and a transparent pathway to re-implement this for the readers, I have decided to include this section to breakdown the implementation details of this new modified VGG network with hybrid convolution.

So we ended up making it that the hybrid layers could take batch inputs; here is the stack of logic:

In forward pass of the main script, we create an instance of the hybrid VGG16, and during training a minibatch of images and their corresponding covariates (together as a 1d tensor) are passed in simultaneously.

The hybrid model first makes a function call to instantiate a vgg16\_bn imported from the Pytorch library, then loads the pretrained weights. Next, it replaces the first \textit{Conv2d} layer with a hybrid convolutional layer. In the forward pass of this hybrid model, input images and the covariate vector enter the hybrid layer together.

The actual hyrbid Conv2d layer extends the Pytorch \textit{nn.Module} class and modifies the regular 2d convolution operator. Here's how this layer works:

Convolution kernels gets updated per minibatch. Each convolution filter consists of two weight tensors, $W_0$ and $W_1$, with Kaiming initialization [reference], plus the covariate input. The value of the weight tensor is ultimately evaluated to either $W_0$ or $W_0+W_1$ depending on whether the covariate is 0 or 1 for a particular image. 

As discussed, the input covariate vector is an array of covariates (with `male' being encoded to 1 and `female' being 0). The \textit{i}th covariate is therefore just a scalar. 

In the forward pass, we have a for loop that iterates over each data point in the minibatch. For each data point, kernel is computed as $k_i = W_0 + W_1 \cdot S_i$ where it is a scalar multiplication.

Then we expand the first dimension of the image to make it has shape (1, 3, 224, 224)

Then a standard 2D convolution is done with the kernel k\_i and a single image x[i]. Then after processing each data point in the minibatch, all the outputs of the Conv2d are concatenated into one. This output is the final output of the hybrid layer and then it becomes the input of the BatchNorm2d layer.

\subsubsection{Experiments and Results}


\subsection{Tuning Batch Size and Learning Rates}


\subsection{Multiple Covariates and Latefuse}

\section{Conclusion}

\subsection{Discussions and Future Work}
This thesis project mainly investigated the creation of hybrid convolution with structured covariate and a practical implementation of Shortfused CNNs. This idea of bringing structured information into CNNs originated from our interest in brain scans and more specifically Alzheimer's detection/automatic diagnosis. Despite the 3-dimensional, volumetric nature of MRI data, I believe that it will not take too much effort to corporate my hybrid layers into the 3D convolutional network we obtain in the lab that is specialized in processing MRI images. 

\section{References}









\end{document}